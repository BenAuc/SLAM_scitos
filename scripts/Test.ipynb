{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e82c2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Localization\n",
    "from Localization import KalmanFilter\n",
    "from Localization import NoiseModel\n",
    "from Localization import MotionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b8a7449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm as norm\n",
    "from numpy.linalg import det as matrix_det\n",
    "from numpy.linalg import inv as matrix_inv\n",
    "from numpy import arctan2 as atan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f346806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor covariance matrix :  [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "filt = KalmanFilter(0.05, [0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d6678f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_features = np.arange(12).reshape((4,3)) + .1\n",
    "z_i = np.arange(2,17).reshape((5,3)) /3\n",
    "last_state_mu = [[2.2], [5.3], [0.7]]\n",
    "k = np.shape(map_features)[0]\n",
    "jacobian_H = np.zeros((4, 3, 3))\n",
    "innovation_S = np.zeros((k, 3, 3))\n",
    "sensor_covariance = np.diag([1, 1, 1])\n",
    "last_covariance = np.arange(1,10).reshape((3,3)) /3\n",
    "map_features\n",
    "last_covariance\n",
    "z_i\n",
    "np.shape(z_i)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "42df4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_x = (map_features[:, 0] - last_state_mu[0])\n",
    "delta_y = (map_features[:, 1] - last_state_mu[1])\n",
    "np.array([delta_x, delta_y])\n",
    "norm(np.array([delta_x, delta_y]), axis=0)\n",
    "z_hat[:, 0] = norm(np.array([delta_x, delta_y]), axis=0)\n",
    "# z_hat = np.power(delta_x ** 2 delta_y ** 2)\n",
    "# z_hat[:, 0] = norm(map_features[:, :2], axis=1)\n",
    "# delta_x = (map_features[:, 0] - last_state_mu[0])\n",
    "# delta_y = (map_features[:, 1] - last_state_mu[1])\n",
    "# z_hat[:, 0].reshape(k, 1)\n",
    "# delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "98e6ea93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.69574275, 1.5       , 4.29534632, 8.40535544])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hat[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2d827594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.90125067, -0.17509595, -0.41664194, -0.50754506])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_H[:, 0, 0] = -1 * np.divide(delta_x, z_hat[:, 0])\n",
    "jacobian_H[:, 0, 1] = -1 * np.divide(delta_y, z_hat[:, 0])\n",
    "jacobian_H\n",
    "-1 * np.divide(delta_x, z_hat[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "688591e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.90125067,  3.80250133,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.17509595,  0.23346127,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.41664194, -0.19229628,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.50754506, -0.35307483,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cfff62a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.73444394, -1.62729522, -0.26759222, -0.092198  ])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hat[:, 1] = atan2(delta_y, delta_x) - last_state_mu[2]\n",
    "z_hat[:, 1]\n",
    "# atan2(delta_y, delta_x)- last_state_mu[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "01a9ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_H[:, 1, 0] = np.divide(delta_y, np.power(delta_x, 2) + np.power(delta_y, 2))\n",
    "jacobian_H[:, 1, 1] = -1 * jacobian_H[:, 1, 0]\n",
    "jacobian_H[:, 1, 2] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b25d2d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1045361 , -2.73444394,  2.1       ],\n",
       "       [ 5.14003891, -1.62729522,  5.1       ],\n",
       "       [ 9.36055554, -0.26759222,  8.1       ],\n",
       "       [13.59485197, -0.092198  , 11.1       ]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_state_mu\n",
    "z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "19c3edec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 40.76229508,  -9.14411035,   0.        ],\n",
       "        [-13.48982616,   3.74603175,   0.        ],\n",
       "        [  0.        ,   0.        ,   1.        ]],\n",
       "\n",
       "       [[  1.01930356,  -0.28145053,   0.        ],\n",
       "        [ -0.18287799,   3.28888889,   0.        ],\n",
       "        [  0.        ,   0.        ,   1.        ]],\n",
       "\n",
       "       [[  1.27973066,   0.82103736,   0.        ],\n",
       "        [  1.54436321,   4.1300813 ,   0.        ],\n",
       "        [  0.        ,   0.        ,   1.        ]],\n",
       "\n",
       "       [[  1.65203982,   1.23318505,   0.        ],\n",
       "        [  2.18427568,   4.0905874 ,   0.        ],\n",
       "        [  0.        ,   0.        ,   1.        ]]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innovation_S = jacobian_H @ last_covariance @ np.transpose(jacobian_H, axes=[0, 2, 1]) + sensor_covariance\n",
    "innovation_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b5c3a6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.3443924 ,  3.30090504,  4.01741175,  4.06419719])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_scores = np.power(2 * np.pi * matrix_det(innovation_S), -0.5)\n",
    "likelihood_scores\n",
    "matrix_det(innovation_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "613148e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.1276575 ,  0.31161355,  0.        ],\n",
       "        [ 0.45970712,  1.38909999,  0.        ],\n",
       "        [ 0.        ,  0.        ,  1.        ]],\n",
       "\n",
       "       [[ 0.99635974,  0.08526465,  0.        ],\n",
       "        [ 0.05540238,  0.30879518,  0.        ],\n",
       "        [ 0.        ,  0.        ,  1.        ]],\n",
       "\n",
       "       [[ 1.02804531, -0.20436973,  0.        ],\n",
       "        [-0.38441746,  0.31854605, -0.        ],\n",
       "        [ 0.        ,  0.        ,  1.        ]],\n",
       "\n",
       "       [[ 1.00649334, -0.30342648,  0.        ],\n",
       "        [-0.53744333,  0.40648614, -0.        ],\n",
       "        [ 0.        ,  0.        ,  1.        ]]])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innovation_S_inv = matrix_inv(innovation_S)\n",
    "innovation_S_inv\n",
    "# z_i[0] - z_hat[0]\n",
    "# innovation_S_inv[0, :, :]\n",
    "# (z_i[0] - z_hat[0])\n",
    "# # (z_i[0] - z_hat[0]) @ innovation_S_inv[0, :, :]\n",
    "# # (z_i[0] - z_hat) @ innovation_S_inv\n",
    "# # np.tensordot((z_i[0] - z_hat),innovation_S_inv, axes=([1,2],[1,2]))\n",
    "# np.tensordot((z_i[0] - z_hat[0]), innovation_S_inv, axes=([0, 1], [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "63520c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.32988462e-06 6.72005709e-09 9.18383317e-30 2.34686119e-61]\n",
      "[4.36094222e-09 3.73120063e-06 1.89149911e-24 3.69692655e-54]\n",
      "[1.12142613e-13 1.79521829e-04 6.71712834e-20 1.20902613e-47]\n",
      "[1.07638295e-19 7.48477844e-04 4.11297906e-16 8.20865486e-42]\n",
      "[3.85628166e-27 2.70416791e-04 4.34234873e-13 1.15704367e-36]\n",
      "[0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "number_obs = np.shape(z_i)[0]\n",
    "number_pred = np.shape(z_hat)[0]\n",
    "likelihood_scores = np.zeros(number_obs)\n",
    "number_pred\n",
    "z_hat[0, :]\n",
    "z_i[0]\n",
    "\n",
    "# for each observed feature a likelihood score is computed w.r.t. each predicted feature\n",
    "\n",
    "for observation in range(number_obs):\n",
    "    scores = np.zeros(number_pred)\n",
    "    \n",
    "    for prediction in range(number_pred):\n",
    "        delta_z = z_i[observation, :] - z_hat[prediction, :]\n",
    "        scores[prediction] = scaling_factor[prediction] * np.exp(-0.5 * delta_z @ innovation_S_inv[prediction, :, :] @ delta_z.T)\n",
    "    print(scores)\n",
    "    likelihood_scores[observation] = np.argmax(scores)\n",
    "print(likelihood_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "cd857332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 1.         1.33333333]\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 39\n"
     ]
    }
   ],
   "source": [
    "### compute the likelihood score ###\n",
    "# pre-compute scaling factor of formula and inverted innovation matrix upfront\n",
    "scaling_factor = np.power(2 * np.pi * matrix_det(innovation_S), -0.5)\n",
    "innovation_S_inv = matrix_inv(innovation_S)\n",
    "\n",
    "# for each pair of observed feature and predicted feature a likelihood score is computed\n",
    "for observation in range(np.shape(z_i)[0]):\n",
    "    print(z_i[observation])\n",
    "    delta_z = z_i[observation] - z_hat\n",
    "#     print(delta_z)\n",
    "    max_score = scaling_factor * np.exp(-0.5 * delta_z @ innovation_S_inv @ delta_z.T)\n",
    "# likelihood_scores[observation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a030f0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07364572, 0.21958039, 0.19903841, 0.19788947])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5659b163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[2, 2], [1, 1]], [[2, 3], [1, 1]]])\n",
    "b= matrix_det(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fe7474cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[-1,  3],\n",
       "        [ 1, -2]]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innovation_S_inv = np.zeros_like(a)\n",
    "for prediction in range(np.shape(a)[0]):\n",
    "    try:\n",
    "        innovation_S_inv[prediction, :, :] = matrix_inv(a[prediction, :, :])\n",
    "    except np.linalg.LinAlgError:\n",
    "        innovation_S_inv[prediction, :, :] = np.eye(np.shape(a)[1])\n",
    "        \n",
    "#         innovation_S_inv[prediction, :, :] = np.ones_like(innovation_S_inv[prediction, :, :])\n",
    "innovation_S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "89250165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_obs = np.shape(z_i)[0]\n",
    "likelihood_scores = np.zeros(number_obs)\n",
    "likelihood_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def correctionStep(self, map_features, z_i):\n",
    "        \"\"\"\n",
    "        This method predicts the features measured by the range finder given a pose estimate\n",
    "        @param:\n",
    "            *map_features: numpy array of dim (k, 3) containing a subset of k features from the map, which are good\n",
    "            candidates that may be observed given current robot pose, and where axis 1 contains in order m_x, m_y, m_s\n",
    "\n",
    "            *z_i: numpy array of dim (i, 3) containing i features extracted from the laser readings,\n",
    "            where the axis 1 contains in order r, phi, s\n",
    "\n",
    "        @result: the method returns:\n",
    "            *z_hat: numpy array of dim (k, 3) containing the predicted measurements,\n",
    "            where the axis 1 contains in order r, phi, s\n",
    "\n",
    "            *jacobian_H: numpy array of dim (k, 3, 3) containing the jacobian of the predicted measurements\n",
    "\n",
    "            *innovation_S: numpy array of dim (k, 3, 3) containing the innovation matrix of the predicted measurements\n",
    "\n",
    "            *self.last_covariance: covariance of state variables corrected by the measurements\n",
    "\n",
    "            *self.last_state_mu: state estimate corrected by the measurements\n",
    "        \"\"\"\n",
    "        ### initialize matrices and indices ###\n",
    "\n",
    "        # number of predictions to be computed\n",
    "        number_pred = np.shape(map_features)[0]\n",
    "\n",
    "        # number of observations made\n",
    "        number_obs = np.shape(z_i)[0]\n",
    "\n",
    "        # z_hat: numpy array of dim (k, 3) containing the predicted measurements\n",
    "        z_hat = np.zeros_like(number_pred)\n",
    "\n",
    "        # jacobian_H: numpy array of dim (k, 3, 3) containing the jacobian of the predicted measurements\n",
    "        jacobian_H = np.zeros((number_pred, 3, 3))\n",
    "\n",
    "        # innovation_S: numpy array of dim (k, 3, 3) containing the innovation matrix of the predicted measurements\n",
    "        innovation_S = np.zeros((number_pred, 3, 3))\n",
    "\n",
    "        ### compute predicted measurements and corresponding jacobian ###\n",
    "\n",
    "        # compute r of each predicted measurements\n",
    "        # norm-2 of the vector from robot's pose to landmark\n",
    "        delta_x = (map_features[:, 0] - self.last_state_mu[0])\n",
    "        delta_y = (map_features[:, 1] - self.last_state_mu[1])\n",
    "        z_hat[:, 0] = norm(np.array([delta_x, delta_y]), axis=0)\n",
    "\n",
    "        # compute partial derivatives of r\n",
    "        # dr/du_x = 0.5 * (1/r) * -2 * (m_x - u_x)\n",
    "        # dr/du_y = 0.5 * (1/r) * -2 * (m_y - u_y)\n",
    "        jacobian_H[:, 0, 0] = -1 * np.divide(delta_x, z_hat[:, 0])\n",
    "        jacobian_H[:, 0, 1] = -1 * np.divide(delta_y, z_hat[:, 0])\n",
    "\n",
    "        # compute phi\n",
    "        z_hat[:, 1] = atan2(delta_y, delta_x) - self.last_state_mu[2]\n",
    "\n",
    "        # compute partial derivatives of phi as per the chain rule\n",
    "        # and the formula of the partial derivatives given on wikipedia: https://en.wikipedia.org/wiki/Atan2\n",
    "        # dphi / du_x = datan2 / ddelta_x * ddelta_x / du_x = -1 * delta_y / (delta_x^2 + delta_y^2) * -1 = delta_y / (delta_x^2 + delta_y^2)\n",
    "        # dphi / du_y = datan2 / ddelta_y * ddelta_y / du_y = delta_y / (delta_x^2 + delta_y^2) * -1 = -1 * dphi / du_x\n",
    "        # dphi / du_psi = -1\n",
    "        jacobian_H[:, 1, 0] = np.divide(delta_y, np.power(delta_x, 2) + np.power(delta_y, 2))\n",
    "        jacobian_H[:, 1, 1] = -1 * jacobian_H[:, 1, 0]\n",
    "        jacobian_H[:, 1, 2] = -1\n",
    "\n",
    "        # compute s\n",
    "        z_hat[:, 2] = map_features[:, 2]\n",
    "\n",
    "        ### compute innovation matrices and initialize its inverse ###\n",
    "\n",
    "        jacobian_H_transposed = np.transpose(jacobian_H, axes=[0, 2, 1])\n",
    "        innovation_S = jacobian_H @ self.last_covariance @ jacobian_H_transposed + self.sensor_covariance\n",
    "        innovation_S_inv = np.zeros_like(innovation_S)\n",
    "\n",
    "        ### compute the likelihood score ###\n",
    "\n",
    "        # pre-compute scaling factor of formula upfront\n",
    "        determinant = matrix_det(innovation_S)\n",
    "        # if determinant = 0 we set to 1 to avoid division by zero\n",
    "        determinant[determinant == 0] = 1\n",
    "        scaling_factor = np.power(2 * np.pi * matrix_det(determinant), -0.5)\n",
    "\n",
    "        # pre-compute inverted innovation matrix upfront\n",
    "        # catch error if matrix can't be inverted\n",
    "        for prediction in range(np.shape(z_hat)[0]):\n",
    "            try:\n",
    "                innovation_S_inv[prediction, :, :] = matrix_inv(innovation_S[prediction, :, :])\n",
    "            except numpy.linalg.LinAlgError:\n",
    "                innovation_S_inv[prediction, :, :] = np.eye(np.shape(innovation_S_inv)[1])\n",
    "\n",
    "        # for each observed feature a likelihood score is computed w.r.t. each predicted feature\n",
    "        # t\n",
    "        for observation_idx in range(number_obs):\n",
    "            scores = np.zeros_like(number_pred)\n",
    "            observation = z_i[observation_idx, :]\n",
    "\n",
    "            for prediction_idx in range(number_pred):\n",
    "                prediction = z_hat[prediction_idx, :]\n",
    "                delta_z = observation - prediction\n",
    "\n",
    "                # a set of likelihood scores is computed for each observation\n",
    "                scores[prediction_idx] = scaling_factor[prediction_idx] * \\\n",
    "                                     np.exp(-0.5 * delta_z @ innovation_S_inv[prediction_idx, :, :] @ delta_z.T)\n",
    "\n",
    "            # for each observed feature the index of the most likely among k features is retained\n",
    "            most_likely_feature = np.argmax(scores)\n",
    "\n",
    "            # compute Kalman gain for this observation\n",
    "            kalman_gain = self.last_covariance @ jacobian_H[most_likely_feature, :, :].T @ innovation_S_inv[most_likely_feature, :, :]\n",
    "\n",
    "            # correct pose and covariance with respect to this observation\n",
    "            self.last_state_mu += kalman_gain * (observation - z_hat[most_likely_feature, :])\n",
    "            self.last_covariance = (np.eye(3) - kalman_gain * jacobian_H[most_likely_feature, :, :]) @ self.last_covariance\n",
    "\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
